\section{Conjugate Analysis on Bernulli Sample}
\newcommand{\Lagr}{\mathcal{L}}

\subsection{Parametric Space of Intrest}
We define the Parametric Space of Intrest as: 

\textbf{Definition 1.1} A \textit{parametric space} is the set of all possible combinations of values for all the different parameters contained in a particular mathematical model.

Given the previous definition the parametric space of interest, which is the probability of success of a single event, is $\Theta \in [0,1]$

\subsection{Ingredients for Bayesian Inference}

\subsubsection{Likelihood Function}

The likelihood function for a series of Bernoulli experiment could be considered as a Binomial distribution, hence given the definition of likelihood:

\textbf{Definition 1.2} The \textit{likelihood function} of a set of parameter values, Î¸, given outcomes x, is equal to the probability of those observed outcomes given those parameter values. More specifically we define the likelihoodFor a iid sample in the following way:

\begin{equation}
\mathcal{L}_{x_1,...X_n}(\theta|x)=P(x|\theta) = \prod_{i=1}^{10} f{(x_i|\theta)}
\end{equation}

Since our idd sample is generated by $X_i \sim Ber(p)$ which $f(x|\theta) = p$, therefore our 10 Bernoulli Trials' likelihood is:

\begin{equation}
\mathcal{L}_{\textbf{x}}(\theta)= \prod_{i=1}^{10} \theta^{x_i}(1 - \theta)^{1 - x_i}=\theta^3(1-\theta)^7
\end{equation}

since in our case we have had 3 successess and 7 failures.

\subsubsection{Prior Distribution}

Since we have no particular information about the experiment we should use a non-informative prior, which ever distribution we use in the modelling.

Another important point to take into account before choosing the prior is that our process is generated by $X_i \sim Ber(p)$.This means that we are able to use conjugate analysis in order to choose a suitable prior.

Given those two element we know that given a $X_i \sim Ber(p)$ likelihood function and $X_i \sim Beta(s_1,s_2)$ as a prior, our postetior distribution would be distributed as a prior. Moreover, taking into account the requirement of non-informativeness of the prior we should use a Beta(0.5, 0.5).


\subsubsection{Posterior Distribution}

The core of Bayesian Inference is knowledge update via Bayes Theorem

\textbf{Theorem 1.1} The\textit{Bayes Theorem} states posterior probability of an event is the consequence of two antecedents, a prior probability and a "likelihood function" derived from a statistical model for the observed data.

Thanks to this theorem we are able to update knowledge about the parameter space $\theta$. 


\begin{equation}
\pi(\theta|x) \propto \theta^3 (1-\theta)^7 \cdot \theta^{-0.5} (1-\theta)^{-0.5} = \theta^{3.5}\theta^{7.5}
\end{equation}

which is, as expected, $\pi(\theta|\textbf{x}) \sim Beta(3.5, 7.5)$

\subsection(Bayesian Inference)

Now that we have a computed our posterior distribution we are able to perform bayesian inference using Beta(3.5, 7.5).
Just to demosntrare the possibility we compute the following 

















